{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\"> Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Eskndrea imports\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer, MosesPunctNormalizer\n",
    "from fairseq.models.transformer import TransformerModel\n",
    "import torch\n",
    "from apply_subword_nmt_bpe import BPE\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_dir = '/home/mmaher/trans_model_eskndrea/'\n",
    "model = TransformerModel.from_pretrained(  \n",
    "    model_dir, # model_name_or_path                                                                                    \n",
    "    checkpoint_file='model.pt',\n",
    "    data_name_or_path=model_dir,\n",
    "    user_dir=f\"{model_dir}/mcolt\",\n",
    "    task=\"translation\",\n",
    "    source_lang=\"LANG_TOK_AR\",\n",
    "    target_lang=\"LANG_TOK_EN\"\n",
    ")\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Camel imports\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "tokenizer_tshkel = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
    "tokenizer_no_tshkel = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
    "mle_egy = MLEDisambiguator.pretrained('calima-egy-r13')\n",
    "tokenizer_egy = MorphologicalTokenizer(mle_egy, scheme='d3tok', split=True)\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# google trans imports\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "english_words = words.words()\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# spacy imports\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# String imports\n",
    "import string, os, re, inflect\n",
    "punctuations = string.punctuation\n",
    "punctuations = punctuations+\"؟\"\n",
    "punctuations = punctuations+\"“\"\n",
    "punctuations = punctuations+\"”\"\n",
    "punctuations = punctuations+\"’\"\n",
    "\n",
    "singularize = inflect.engine()\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# General imports\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mmaher/LM_CS/Franco/data_all_final.txt\"\n",
    "with open(path) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [l.replace(\"\\n\", \"\") for l in  lines]\n",
    "\n",
    "lines_ar = lines[2000:2500]\n",
    "lines_ar = [i.replace(\"\\n\",\"\").strip() for i in lines_ar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_en_google = list(pd.read_excel(\"/home/mmaher/LM_CS/trans_methods/google_trans/sents/en/all.xlsx\")[0][2000:2500])\n",
    "\n",
    "with open(\"/home/mmaher/LM_CS/trans_methods/google_trans/words/ar/0_1M_words_ar.txt\") as f:\n",
    "    words_ar = f.readlines()\n",
    "words_ar_google = [i.replace(\"\\n\",\"\").strip() for i in words_ar]\n",
    "\n",
    "with open(\"/home/mmaher/LM_CS/trans_methods/google_trans/words/en/0_1M_words_en.txt\") as f:\n",
    "    words_en = f.readlines()\n",
    "words_en_google = [i.replace(\"\\n\",\"\").strip() for i in words_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eskndrea data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/mmaher/LM_CS/trans_methods/esk_trans/sents/en/2K_2.5K_en_sents.txt\") as f:\n",
    "    lines_en_eskndrea = f.readlines()\n",
    "\n",
    "with open(\"/home/mmaher/LM_CS/trans_methods/esk_trans/words/ar/0_1M_words_ar.txt\") as f:\n",
    "    words_ar_esk = f.readlines()\n",
    "words_ar_eskndrea = [i.replace(\"\\n\",\"\").strip().lower() for i in words_ar_esk]\n",
    "\n",
    "with open(\"/home/mmaher/LM_CS/trans_methods/esk_trans/words/en/0_1M_words_ar.txt.out.ar-en.en\") as f:\n",
    "    words_en = f.readlines()\n",
    "words_en_eskndrea = [i.replace(\"\\n\",\"\").strip().lower() for i in words_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main line       : ده مكتوب فى العدد الأخير من مجلة الأيكونوميست تأكدت\n",
      "google trans    :  This is written in the latest issue of The Economist\n",
      "eskndrea trans  : It was written in the last issue of the magazine and it was verified.\n",
      "\n",
      "\n",
      "Main line       : وألا يتحول أى اتفاق فى وجهات النظر إلى\n",
      "google trans    :  And that no agreement in viewpoints turns into\n",
      "eskndrea trans  : And no deal turns up in my sights.\n",
      "\n",
      "\n",
      "Main line       : جديدة داخل عقل وقلب كل واحد فينا أولا مصر تستلهم روح خمسة و عشرون يناير\n",
      "google trans    :  New inside the mind and heart of each one of us First Egypt is inspired by the spirit of January 25\n",
      "eskndrea trans  : Inside the mind and heart of every one, Egypt receives the spirit of January 25\n",
      "\n",
      "\n",
      "Main line       : فئة ثانية أيدت خمسة و عشرون وأيدت ثلاثون لكنها\n",
      "google trans    :  A second category supported twenty-five and supported thirty, but it\n",
      "eskndrea trans  : Class III: Twentyfive and thirty but none\n",
      "\n",
      "\n",
      "Main line       : وتبعاتها واعتبروا أن الخامس والعشرين من\n",
      "google trans    :  and its consequences, and they considered that the twenty-fifth of\n",
      "eskndrea trans  : Therefore, consider the 25th.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,50,10):\n",
    "    print(\"Main line       : \" + lines_ar[i])\n",
    "    print(\"google trans    : \" + lines_en_google[i])\n",
    "    print(\"eskndrea trans  : \" + lines_en_eskndrea[i]+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\"> Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_English(string_test):\n",
    "    string_test_list = string_test.strip().split(\" \")\n",
    "    \n",
    "    for s in string_test_list:\n",
    "        if(len(s)>0):\n",
    "            s = rmv_punct(s, True)\n",
    "            if((singularize.singular_noun(s) in english_words) or (s in english_words)  or (s[:-1] in english_words) or ((s[:-2] in english_words) and (s[-2:]=='ed') )):\n",
    "                pass\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def reverse_order(string_mix, dict_, string_en):\n",
    "\n",
    "    dict_values = list(dict_.values())\n",
    "\n",
    "    for k in range(len(dict_values)):\n",
    "        if ((dict_values[k] + \" \" + dict_values[k-1]) in string_en):\n",
    "            asl = dict_values[k-1] + \" \" + dict_values[k]\n",
    "            b3d = dict_values[k] + \" \" + dict_values[k-1]\n",
    "            string_mix = ( re.sub(rf'\\b{asl}\\b',b3d, string_mix  ))\n",
    "\n",
    "    return string_mix\n",
    "\n",
    "def morph_tokenize(s):\n",
    "    sentence = simple_word_tokenize(s)\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "def tfkek_l_asl(s_ar):\n",
    "    sennt = simple_word_tokenize(s_ar)\n",
    "    tokens = tokenizer_no_tshkel.tokenize(sennt)\n",
    "    tokens[0] = tokens[0].replace(\"+\",\"\")\n",
    "    \n",
    "    tokens2 = tokens\n",
    "    for l in range(len(tokens)-1):\n",
    "        if(tokens[l]==\"ل\" and tokens[l+1]== 'ال+'):\n",
    "            tokens2.remove(tokens[l])\n",
    "            tokens2.remove(tokens[l])\n",
    "            tokens2.append(\"لل\")\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "def tfkek_l_asl_egy(s_ar):\n",
    "    sennt = simple_word_tokenize(s_ar)\n",
    "    tokens = tokenizer_egy.tokenize(sennt)\n",
    "    tokens[0] = tokens[0].replace(\"+\",\"\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def shkl(str_no_tshkel):\n",
    "    skip = False\n",
    "    str_no_tshkel_tfkek = tfkek_l_asl(str_no_tshkel , True)\n",
    "    s = \"\"\n",
    "    for i in range(len(str_no_tshkel_tfkek)):\n",
    "        if(not skip):\n",
    "            if((str_no_tshkel_tfkek[i][0]==\"+\") ):\n",
    "                s = s + str_no_tshkel_tfkek[i].replace(\"+\",\"\") \n",
    "\n",
    "            elif((str_no_tshkel_tfkek[i][-1]==\"+\") or (str_no_tshkel_tfkek[i]== 'ال')):\n",
    "                s = s + \" \" +  str_no_tshkel_tfkek[i].replace(\"+\",\"\") + str_no_tshkel_tfkek[i+1]\n",
    "                skip = True\n",
    "\n",
    "            else:\n",
    "                s = s + \" \" + str_no_tshkel_tfkek[i]\n",
    "        else:\n",
    "            skip = False\n",
    "    return s.strip()\n",
    "   \n",
    "\n",
    "def translate_fun_google(s):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(s).text # Translate\n",
    "    return translated_text\n",
    "\n",
    "def translate_fun_google_ar(s):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(s,dest=\"ar\").text # Translate\n",
    "    return translated_text\n",
    "\n",
    "def translate_fun(which, text):\n",
    "\n",
    "    if(which==\"eskndra\"):\n",
    "        return trans_ar_en_eskndrea(text)\n",
    "        \n",
    "    else:\n",
    "        return translate_fun_google(text)\n",
    "\n",
    "possessive_determiner = [\"my\",\"your\",\"his\",\"her\",\"its\",\"our\",\"their\",\"one's\", \"whose\"]\n",
    "def rmv_pssv(stri, sent):\n",
    "    found  =  False\n",
    "    if(stri in sent):\n",
    "        return stri , found\n",
    "    for i in possessive_determiner:\n",
    "        if(re.findall(rf'\\b{i}\\b',stri)):\n",
    "            stri = re.sub(rf'\\b{i}\\b',\"\",stri) \n",
    "            found = True\n",
    "        \n",
    "    return stri.strip(), found\n",
    "\n",
    "def add_zeada(stri):\n",
    "    zeadat = [\"بت\",\"ن\",\"بي\",\"فت\",\"ي\",\"مت\",\"ت\",\"ه\",\"فلي\",\"هات\",\"ف\",\"تُ\",\"وي\",\"يت\", \"أ\",\"ا\",\"بن\"]\n",
    "    zeada = \"\"\n",
    "    for s in range(len(stri)):\n",
    "        if(((stri[0:s+1] in zeadat) and (stri[0:s+2] not in zeadat) )):\n",
    "            zeada = stri[0:s+1]\n",
    "    return zeada\n",
    "\n",
    "def check_verb(verb_ar):\n",
    "    sentence = simple_word_tokenize(verb_ar)\n",
    "    disambig = mle_egy.disambiguate(sentence)\n",
    "    pos_tags = [d.analyses[0].analysis['pos'] for d in disambig][0]\n",
    "    return pos_tags==\"verb\"\n",
    "\n",
    "def check_verb_en(verb_en):\n",
    "    sen = sp(verb_en)\n",
    "    try:\n",
    "        if(sen[0].pos_.lower()=='verb'): \n",
    "            return True\n",
    "\n",
    "        if(verb_en[-1]==\"s\"):\n",
    "            sen = sp(verb_en[:-1])#rmv s like plays\n",
    "            if(sen[0].pos_.lower()=='verb'): \n",
    "                return True\n",
    "\n",
    "        if(verb_en[-3:]==\"ing\"):\n",
    "            sen = sp(verb_en[:-3])#rmv ing like playing\n",
    "            if(sen[0].pos_.lower()=='verb'): \n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def z7z7_punct(stri):\n",
    "    for i in punctuations: \n",
    "        if(i in stri): stri =  stri.replace(i,\" \"+ i + \" \")\n",
    "    return stri  \n",
    "\n",
    "def al_and_arrang(s2):\n",
    "    sent_split = s2.split()\n",
    "    for i in range(len(sent_split)):\n",
    "        if((i < len(sent_split)-1 ) and ((sent_split[i]==\"ال\") and (sent_split[i+1]==\"و\"))):\n",
    "            sent_split[i] = \"و\"\n",
    "            sent_split[i+1] = \"ال\"\n",
    "    return \" \".join(sent_split)\n",
    "\n",
    "def rmv_repeated(sent_list):\n",
    "    sent_list = (\" \".join(sent_list)).split(\" \")\n",
    "    sent_list2 = sent_list.copy()\n",
    "    for i in range(len(sent_list)-1):\n",
    "        try:\n",
    "            if(is_English(sent_list[i]) and (not is_English(sent_list[i+1])) ):\n",
    "                if(sent_list[i] == ((translate_fun_google(sent_list[i+1]).split(\" \"))[0]).lower()):\n",
    "                    sent_list2.remove(sent_list[i])\n",
    "            if(not is_English(sent_list[i]) and (is_English(sent_list[i+1])) ):\n",
    "                if((translate_fun_google(sent_list[i]).split(\" \")[0]).lower() ==  sent_list[i+1]):\n",
    "                    sent_list2.remove(sent_list[i+1])\n",
    "        except:\n",
    "            pass\n",
    "    return  \" \".join(sent_list2)\n",
    "\n",
    "pronouns_ar = [\"أ\",\"ت\",\"ن\",\"+وا\",\"ي\",\"ت\",\"\",\"\",\"\",\"\",\"\",\"\"]\n",
    "def b_al_pronouns_arrange(s):\n",
    "    sent_list = s.lower().split(\" \")\n",
    "    sent_list = [i for i in sent_list if(i not in ['', ' '])]\n",
    "\n",
    "    sent_list = (\" \".join(sent_list)).split(\" \")\n",
    "    for i in range(len(sent_list)-1):\n",
    "        try:\n",
    "            if((sent_list[i] == \"ال\") and ((sent_list[i+1]) in pronouns ) ):\n",
    "                sent_list[i] = sent_list[i+1]\n",
    "                sent_list[i+1] = \"ال\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i in range(len(sent_list)-1):\n",
    "        try:\n",
    "            if((sent_list[i] in pronouns) and ((sent_list[i+1]) == \"و\" ) ):\n",
    "                sent_list[i+1] = sent_list[i]\n",
    "                sent_list[i] = \"و\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i in range(len(sent_list)-1):\n",
    "        try:\n",
    "            if((sent_list[i] == \"ب\") and ((sent_list[i+1]) in pronouns ) ):\n",
    "                if( (sent_list[i+1]) != \"they\"):\n",
    "                    sent_list[i+1] = pronouns_ar[pronouns.index(sent_list[i+1])]\n",
    "                else:\n",
    "                    sent_list[i+1] = \" ي \"  + sent_list[i+2]\n",
    "                    sent_list[i+2] = \"وا\" \n",
    "        except:\n",
    "            pass\n",
    "    return  \" \".join(sent_list)\n",
    "      \n",
    "def check_pos_pronoun(en, s, i):\n",
    "    en_list = en.split(\" \")\n",
    "    if(check_verb_en(en_list[i-2]) or (en_list[i-2] in [\"am\", \"is\", \"are\", \"was\", \"were\",\"about\", \"against\"])):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_index(sent, w):\n",
    "    i = (re.search(rf'\\b{w}\\b', sent)).span()[0]\n",
    "    spaces = 0\n",
    "    for i in range(0,i):\n",
    "        if(sent[i]==\" \"):\n",
    "            spaces+=1\n",
    "    return spaces\n",
    "\n",
    "def b_w_arrange(s):\n",
    "    sent_list = s.split(\" \")\n",
    "    sent_list = (\" \".join(sent_list)).split(\" \")\n",
    "    for i in range(len(sent_list)-1):\n",
    "        try:\n",
    "            if((sent_list[i] == \"ب\") and ((sent_list[i+1]) == \"و\" ) ):\n",
    "                sent_list[i] = \"و\"\n",
    "                sent_list[i+1] = \"ب\"\n",
    "        except:\n",
    "            pass\n",
    "    return  \" \".join(sent_list)\n",
    "    \n",
    "def rmv_punct(stri, all=False):\n",
    "\n",
    "    for i in punctuations: \n",
    "        try:\n",
    "            if(((i in stri)) and ( (not (( (i==\"'\") and (stri[stri.find(\"'\")+1]==\"s\") ))) or all ) and ( (not (( (i==\"’\") and (stri[stri.find(\"’\")+1]==\"s\") ))) or all ) ): \n",
    "                stri =  stri.replace(i,\"\")\n",
    "        except:\n",
    "            if(((i in stri))  ): \n",
    "                stri =  stri.replace(i,\"\")\n",
    "\n",
    "    stri = stri.replace(\"'s\",\" 's \").replace(\"’s\",\" ’s\")\n",
    "\n",
    "    return stri\n",
    "\n",
    "\n",
    "def check_pos_tag_en(sent, word):\n",
    "    text = nltk.word_tokenize(sent)\n",
    "    return dict(nltk.pos_tag(text))[word.strip().split(\" \")[0]]\n",
    "\n",
    "def check_ll_after_of(x, en_sent, ar_trans_sent_list, ar_sent_list):\n",
    "    while( x < ( len( en_sent.split(\" \")))):\n",
    "        if( en_sent.split(\" \")[x] not in stopwords.words() ):\n",
    "            f = en_sent.split(\" \")[x]\n",
    "            break\n",
    "        x+=1\n",
    "\n",
    "    for i in range(len(ar_trans_sent_list)):\n",
    "        if( re.search(rf'\\b{f}\\b', ar_trans_sent_list[i] ) ):\n",
    "            if(\"لل\" in tfkek_l_asl(ar_sent_list[i])):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "    \n",
    "def trans_ar_en_eskndrea(text):\n",
    "    src='ar'  ; tgt='LANG_TOK_EN'\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Normalization & punctation\n",
    "    moses_norm = MosesPunctNormalizer(lang=\"ar\",\n",
    "        penn=True,\n",
    "        norm_quote_commas=True,\n",
    "        norm_numbers=True,\n",
    "        pre_replace_unicode_punct=True,\n",
    "        post_remove_control_chars=True\n",
    "    )\n",
    "    moses_tok = MosesTokenizer(lang=\"ar\")\n",
    "\n",
    "    normalized = moses_norm.normalize(text)\n",
    "    tokenized = moses_tok.tokenize(normalized, aggressive_dash_splits=True, escape=True)\n",
    "    tokenized = ' '.join(tokenized)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # BPE\n",
    "    subword_nmt_bpe_preprocessor = BPE(open(f\"/home/mmaher/LM_CS/trans_methods/esk_trans/codes.bpe.32000\", 'r'))\n",
    "    text_subword_nmt_bpe_encoded = subword_nmt_bpe_preprocessor.process_line(tokenized)\n",
    "    text_subword_nmt_bpe_encoded = tgt + \" \" + text_subword_nmt_bpe_encoded\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        translated_output = model.translate(text_subword_nmt_bpe_encoded, beam=5)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Detokenize\n",
    "    moses_detok = MosesDetokenizer(lang=\"en\")\n",
    "    subword_nmt_bpe_decoded = re.sub(r\"(@@ )|(@@ ?$)\", \"\", translated_output)\n",
    "    tokens = subword_nmt_bpe_decoded.split(\" \")\n",
    "    detokenized = moses_detok.detokenize(tokens, return_str=True)\n",
    "\n",
    "    return detokenized\n",
    "\n",
    "def pos_tags_ar(ar):\n",
    "    sentence = simple_word_tokenize(ar)\n",
    "    disambig = mle_egy.disambiguate(sentence)\n",
    "    return [d.analyses[0].analysis['pos'] for d in disambig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\"> align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = [\"i\" , \"you\" , \"we\" , \"they\" , \"he\", \"she\" , \"it\", \"a\" ,\"an\" , \"in\", \"on\", \"at\"]\n",
    "def trans_aligment(ar_sent,en_sent, en_words, ar_words, print_=False, method_trans=\"google\", first = True, last_dict_= {}):\n",
    "    trans_dict = {}\n",
    "    enter =True ; al_in_tokens = False ; b_in_tokens = False; and_in_tokens = False; \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # move punctuations away from words for correct searching in \".index\" ex: civilian?\n",
    "    ar_sent = rmv_punct(ar_sent)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # translate by CONTEXT\n",
    "    en_sent = rmv_punct(en_sent.lower()).strip()   # ar trans to en\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # translate by WORD\n",
    "    ar_sent_list = [i  for i in word_tokenize(ar_sent)]\n",
    "    \n",
    "    ar_trans_sent_list = []\n",
    "    for i in ar_sent_list:\n",
    "        try:\n",
    "            ar_trans_sent_list.append( rmv_punct(en_words[ar_words.index(i)].lower()).strip() )\n",
    "        except:\n",
    "            print( i ,\" word not found, consider adding it later plz\")\n",
    "            ar_trans_sent_list.append( rmv_punct( translate_fun( which= method_trans , text = i).lower() ).strip() )\n",
    " \n",
    "    ar_trans_sent = \" \".join(ar_trans_sent_list)\n",
    "    ar_sent_list2 = ar_sent_list.copy()\n",
    "\n",
    " \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # replace ar by en if near\n",
    "    for i in range(len(ar_trans_sent_list)):    \n",
    "        al_in_tokens = False ; b_in_tokens = False; and_in_tokens = False; \n",
    "        search_word, pssv = rmv_pssv(ar_trans_sent_list[i] , en_sent)\n",
    "        search_word = search_word.strip()\n",
    "        \n",
    "        if(\"the\" in search_word.lower().split(\" \")):\n",
    "            al_in_tokens = True\n",
    "            search_word = re.sub(rf'\\b{\"the\"}\\b',\"\",search_word.lower()).strip()\n",
    "        \n",
    "        if((\"by\" in search_word.lower().split(\" \")) or (\"with\" in search_word.lower().split(\" \"))):\n",
    "            b_in_tokens = True\n",
    "            search_word = re.sub(rf'\\b{\"by\"}\\b',\"\",search_word.lower()).strip()\n",
    "            search_word = re.sub(rf'\\b{\"with\"}\\b',\"\",search_word.lower()).strip()\n",
    "\n",
    "        if(((search_word) not in stopwords.words() )  and ((search_word) not in punctuations ) and (re.search(rf'\\b{search_word}\\b', en_sent)) and (is_English(search_word))):\n",
    "        \n",
    "            index_context_trans = check_index(en_sent, search_word)# mkan el translated f context translation\n",
    "            index_word_trans = check_index( ar_trans_sent , search_word)# mkan el translated f word translation\n",
    "            a = ar_sent_list[i]\n",
    "            index_context_ar = ar_sent_list.index(a)\n",
    "\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            if((first) or (a in list(last_dict_.keys()))):\n",
    "                \n",
    "                if((abs(index_context_trans-index_word_trans) <= 4) and (abs(index_context_trans-index_context_ar) <= 4) ):\n",
    "                    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                    # to skip sequentional words\n",
    "                    if(enter):\n",
    "                        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        #check pronouns\n",
    "                        pieces_ar = [p.replace(\"+\",\"\") for p in tfkek_l_asl(a)]\n",
    "                    \n",
    "                        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        # al & b in tokens check\n",
    "                        if((\"ال\" in pieces_ar) and (\"the\" not in search_word.lower().split(\" \")) )  : al_in_tokens = True\n",
    "                        if((\"ب\" in pieces_ar) and (\"by\" not in search_word.lower().split(\" \")) ) : b_in_tokens = True\n",
    "                        if((\"و\" in pieces_ar) and (\"and\" not in search_word.lower().split(\" \")) ) : and_in_tokens = True\n",
    "\n",
    "\n",
    "                        search_word_no_zeada = search_word\n",
    "\n",
    "\n",
    "                        if((len(pieces_ar) <= len(search_word.split(\" \"))) or (and_in_tokens and (len(pieces_ar) <= len(search_word.split(\" \"))+1)) or (al_in_tokens and (len(pieces_ar) <= len(search_word.split(\" \"))+1)) or (b_in_tokens and (len(pieces_ar) <= len(search_word.split(\" \"))+1)) or (b_in_tokens and al_in_tokens and (len(pieces_ar) <= len(search_word.split(\" \"))+2))  or (pssv and (len(pieces_ar) <= len(search_word.split(\" \"))+1)) ):# 3shan lw feh dmaer na2s mn translation\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            # of check\n",
    "                            n = en_sent.index(search_word)\n",
    "                            if(en_sent[n+len(search_word)+1 :  n+len(search_word)+4] == \"of \"):\n",
    "                                if( ( ar_sent_list[i+1] not in [\"من\", \"عن\", \"الى\", \"\", \"\"]) and (\"لل\" not in tfkek_l_asl(ar_sent_list[i+1])) and (\"ل\" not in tfkek_l_asl(ar_sent_list[i+1])) and (\"ب\" not in tfkek_l_asl(ar_sent_list[i+1])) and (pos_tags_ar(ar_sent)[i+1] !=\"adj\") ):\n",
    "                                    search_word = search_word + \" of\"               \n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            # al & b in tokens update\n",
    "                            if(al_in_tokens): search_word = \" ال \" + search_word ;  al_in_tokens = False  # put it after \"of\" check for correct searching\n",
    "                            if(b_in_tokens): search_word = \" ب \" + search_word ; b_in_tokens = False  # put it after \"of\" check for correct searching\n",
    "                            if(and_in_tokens): search_word = \" و \" + search_word ;  and_in_tokens = False  # put it after \"of\" check for correct searching\n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            # The , And check\n",
    "                            search_word =  re.sub(rf'\\b{\"the\"}\\b',\"ال\",search_word) ; search_word =  re.sub(rf'\\b{\"The\"}\\b',\"ال\",search_word)\n",
    "                            search_word =  re.sub(rf'\\b{\"and\"}\\b',\"و\",search_word)  ; search_word =  re.sub(rf'\\b{\"And\"}\\b',\"و\",search_word)\n",
    "\n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            # pronouns & posseisve pronouns check 3shan feh pronouns mbtgesh f trgma by word\n",
    "                            if( (\"ال\" not in search_word) and (search_word.split(\" \")[0] not in pronouns) and ((((en_sent).split(\" \"))[index_context_trans-1] in pronouns) and (((en_sent).split(\" \"))[index_context_trans-1]) not in search_word.split(\" \")) and (index_context_trans!=0)):\n",
    "                                # check mkan el pronoun ze how are you & you are beautiful\n",
    "                                if(check_pos_pronoun(en_sent,search_word, index_context_trans)):\n",
    "                                    #check if adjective not add 3shan beb2a aslha klmtee m2lobeen\n",
    "                                    if( check_pos_tag_en(en_sent,search_word_no_zeada) != 'JJ'):\n",
    "                                        search_word  =  ((en_sent).split(\" \"))[index_context_trans-1] + \" \" + search_word\n",
    "                            \n",
    "\n",
    "                            if( (\"ال\" not in search_word) and ((((en_sent).split(\" \"))[index_context_trans-1] in possessive_determiner) and (((en_sent).split(\" \"))[index_context_trans-1]) not in search_word.split(\" \")) and (index_context_trans!=0)):\n",
    "                                search_word  =  ((en_sent).split(\" \"))[index_context_trans-1] + \" \" + search_word\n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            # check zeadat el af3al\n",
    "                            if((check_verb( a )) and (check_verb_en( (search_word.split(\" \"))[0])) and (search_word[-3:]!=\"ing\") and (search_word[-2:]!=\"ed\")):\n",
    "                                search_word = (add_zeada(a)) + search_word\n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            if(a not in [\"طب\",\"بدرى\",\"بسرعة\",\"فيها\",\"بدري\"]):\n",
    "                                ar_sent_list2[index_context_ar] = search_word\n",
    "                                trans_dict[a]=search_word\n",
    "                                enter = False\n",
    "\n",
    "                        else:\n",
    "                            #print( \"search_word : \" , a , search_word)\n",
    "                            #وأنه and that  ;;;  بحملات campaigns ;;;; بدري early\n",
    "                            pass\n",
    "\n",
    "        else:\n",
    "            enter = True \n",
    "\n",
    "    if(print_):\n",
    "        print(\" ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  context based ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"gomale 3arbe                 : ar_sent             \" , ar_sent)\n",
    "        print(\"trgmt el gomla el aslea      : en_sent             \" , en_sent)\n",
    "\n",
    "        print(\"\\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  word based ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"split el gomla el aslea      : ar_sent_list        \" , ar_sent_list)\n",
    "        print(\"trans word by word           : ar_trans_sent_list \" , ar_trans_sent_list)\n",
    "        print(\"trans word by word           : ar_trans_sent      \" , ar_trans_sent)\n",
    "\n",
    "        print(\"\\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  output ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"dict                         : trans_dict         \" , trans_dict)\n",
    "        print(\"mix out                      : ar_sent_list2        \" , ar_sent_list2)    \n",
    "\n",
    "\n",
    "    return trans_dict, en_sent, ar_sent_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar = 'يصنف كفعل حر'\n",
    "test_en_google = translate_fun_google(test_ar)\n",
    "dict_2, en_sent, s = trans_aligment( ar_sent= test_ar, en_sent= test_en_google, en_words= words_en_google, ar_words=words_ar_google , method_trans=\"google\" , print_= True, first = True, last_dict_= {})\n",
    "s2 = b_w_arrange(b_al_pronouns_arrange(al_and_arrang(rmv_repeated(s)))) #وبال\n",
    "if(s2.split(\" \")[-1].strip()==\"of\"):\n",
    "    s2 = s2.strip()[:-2]\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  context based ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "gomale 3arbe                 : ar_sent              عن الشباب الذى قرر ألا ينخرط فى عمل سياسى أو حزبى\n",
      "trgmt el gomla el aslea      : en_sent              about the youth who decided not to engage in political or party work\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  word based ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "split el gomla el aslea      : ar_sent_list         ['عن', 'الشباب', 'الذى', 'قرر', 'ألا', 'ينخرط', 'فى', 'عمل', 'سياسى', 'أو', 'حزبى']\n",
      "trans word by word           : ar_trans_sent_list  ['about', 'youth', 'which', 'decide', 'alla', 'engage', 'fi', 'work', 'political', 'ow', 'my party']\n",
      "trans word by word           : ar_trans_sent       about youth which decide alla engage fi work political ow my party\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  output ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "dict                         : trans_dict          {'الشباب': ' ال youth', 'ينخرط': 'يengage', 'سياسى': 'political', 'حزبى': 'party'}\n",
      "mix out                      : ar_sent_list2         ['عن', ' ال youth', 'الذى', 'قرر', 'ألا', 'يengage', 'فى', 'عمل', 'political', 'أو', 'party']\n",
      "عن ال youth الذى قرر ألا يengage فى عمل political أو party\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  context based ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "gomale 3arbe                 : ar_sent              عن الشباب الذى قرر ألا ينخرط فى عمل سياسى أو حزبى\n",
      "trgmt el gomla el aslea      : en_sent              on the youth who decided not to engage in political or partisan work\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  word based ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "split el gomla el aslea      : ar_sent_list         ['عن', 'الشباب', 'الذى', 'قرر', 'ألا', 'ينخرط', 'فى', 'عمل', 'سياسى', 'أو', 'حزبى']\n",
      "trans word by word           : ar_trans_sent_list  ['about', 'young', 'that', 'he decided', 'except', 'engage', 'in', 'a job', 'politician', 'or', 'party']\n",
      "trans word by word           : ar_trans_sent       about young that he decided except engage in a job politician or party\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  output ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "dict                         : trans_dict          {'ينخرط': 'يengage'}\n",
      "mix out                      : ar_sent_list2         ['عن', 'الشباب', 'الذى', 'قرر', 'ألا', 'يengage', 'فى', 'عمل', 'سياسى', 'أو', 'حزبى']\n",
      "عن الشباب الذى قرر ألا يengage فى عمل سياسى أو حزبى\n"
     ]
    }
   ],
   "source": [
    "dict_use = False\n",
    "if(dict_use):\n",
    "    i = 205 \n",
    "    test_ar = lines_ar[i]\n",
    "    test_en_esk = lines_en_eskndrea[i]\n",
    "    test_en_google = lines_en_google[i]\n",
    "else:\n",
    "    test_ar = \"عن الشباب الذى قرر ألا ينخرط فى عمل سياسى أو حزبى\"\n",
    "    test_en_google = translate_fun(\"google\", test_ar )\n",
    "    test_en_esk = translate_fun(\"eskndra\", test_ar)\n",
    "\n",
    "dict_, en_sent, s = trans_aligment( ar_sent= test_ar, en_sent= test_en_esk, ar_words = words_ar_eskndrea, en_words = words_en_eskndrea, method_trans=\"eskndra\" , print_= True, first = True, last_dict_= {})\n",
    "s2 = b_w_arrange(b_al_pronouns_arrange(al_and_arrang(rmv_repeated(s))))\n",
    "if(s2.split(\" \")[-1].strip()==\"of\"):\n",
    "    s2 = s2.strip()[:-2]\n",
    "print( s2)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "dict_2, en_sent, s = trans_aligment( ar_sent= test_ar, en_sent= test_en_google, en_words= words_en_google, ar_words=words_ar_google , method_trans=\"google\" , print_= True, first = False, last_dict_= dict_)\n",
    "s2 = b_w_arrange(b_al_pronouns_arrange(al_and_arrang(rmv_repeated(s))))\n",
    "if(s2.split(\" \")[-1].strip()==\"of\"):\n",
    "    s2 = s2.strip()[:-2]\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/500 [00:38<32:42,  4.03s/it]"
     ]
    }
   ],
   "source": [
    "if(False):\n",
    "    ar_en_sent_all = [] ; ar_sent_all = [] ; dicts = [] ; en_sents = []\n",
    "\n",
    "    for i in tqdm(range(len(lines_en_eskndrea))):\n",
    "        ar_sent= lines_ar[i]\n",
    "        dict_, en_sent, s = trans_aligment( ar_sent= lines_ar[i],en_sent= lines_en_eskndrea[i], ar_words = words_ar_eskndrea, en_words = words_en_eskndrea , print_= False, first = True, last_dict_= {})\n",
    "        s2 = b_w_arrange(b_al_pronouns_arrange(al_and_arrang(rmv_repeated(s))))\n",
    "        if(s2.split(\" \")[-1].strip()==\"of\"):\n",
    "            s2 = s2.strip()[:-2]\n",
    "\n",
    "\n",
    "        dict_2, en_sent, s = trans_aligment( ar_sent= lines_ar[i],en_sent= lines_en_google[i], en_words= words_en_google, ar_words=words_ar_google , print_= False, first = False, last_dict_= dict_)\n",
    "        s2 = b_w_arrange(b_al_pronouns_arrange(al_and_arrang(rmv_repeated(s))))\n",
    "        if(s2.split(\" \")[-1].strip()==\"of\"):\n",
    "            s2 = s2.strip()[:-2]\n",
    "\n",
    "        # save\n",
    "        if(len(dict_2)>0):\n",
    "            ar_en_sent_all.append(\"ااا  \" + s2)\n",
    "            ar_sent_all.append(ar_sent)\n",
    "            dicts.append(dict_2)\n",
    "            en_sents.append(en_sent)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.546"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicts)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/mmaher/LM_CS/word_aligment/outs/2K_2.5K_out_CS2.txt\" , \"w\") as textfile:\n",
    "    for e in range(len(ar_en_sent_all)):\n",
    "        textfile.write(ar_sent_all[e]+  \"\\n\")\n",
    "        textfile.write(en_sents[e]+  \"\\n\")\n",
    "        textfile.write(ar_en_sent_all[e]+  \"\\n\")\n",
    "        textfile.write(str(dicts[e]))\n",
    "        textfile.write(\"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_, en_sent, s = trans_aligment(\"بتظهر الرجاله الانانيه\",True,\"google\")#ex: context importance\n",
    "#dict_, en_sent, s = trans_aligment(\"الانانيه صفة وحشة\",True,\"google\")#ex: context importance\n",
    "#dict_, en_sent, s = trans_aligment(\"اكلت عصيرها\",True,\"google\") #ex: add pronouns importance\n",
    "#dict_, en_sent, s = trans_aligment(\"نفسي فى عصيرها\",True,\"google\")#ex: correct posseisve pronouns importance\n",
    "#dict_, en_sent, s = trans_aligment(\"تمام بالقلم\",True,\"google\")#ex: b & al\n",
    "#dict_, en_sent, s = trans_aligment(\" وعينك\",True,\"google\")#ex: and\n",
    "#dict_, en_sent, s = trans_aligment(\"جزء منك كان حلو\",True,\"google\")#ex: of\n",
    "#dict_, en_sent, s = trans_aligment(\"خبر مش جلو\",True,\"google\")#ex: the\n",
    "#dict_, en_sent, s = trans_aligment(\"نعم بياكل نعم فلنلعب نعم يلعب\",True,\"google\")#ex: zeadat el af3al \n",
    "#~~~~~~~~~~\n",
    "#                      مشكله الضمير السابق ناخده و لا لا\n",
    "#  الواحد مش عارف ينام مش عشان خسر من البايرن لا عشان جول شيكابالا\n",
    "#ازيك يا شروق انا حنين صاحبتك فكراني\n",
    "#~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca2ea4dd711042ce916a1f350bb0af1ee06de2cd088216482cff3d6db9d14d59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
